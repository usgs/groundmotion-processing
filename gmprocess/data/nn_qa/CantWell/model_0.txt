
##################################################
Model 0:
	Learning rate:	0.002
	Neuron in HL1:	15
	Neuron in HL2:	15
	Optimizer:	Nadam
	Dropout rate:	0.0
	Activation fx:	sigmoid
	Batch size:	8
	Learning rate decay:	0.0
	L1 regularizer:	0.0
	L2 regularizer:	1e-05
	Patience:	10
--------------------------------------------------
5-fold cross-validation result:	0.07257366013416666
##################################################

--------------------------------------------------
Validation on folder 1:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.47496147295742325	0.22364694243310657
1	0.15422794896770609	0.11541729909743945
2	0.10101727969436483	0.10071857133059708
3	0.08652774009740714	0.10013289622232187
4	0.07895496889073966	0.08946974149312982
5	0.07599389992484992	0.09321466687910816
6	0.07287399200722575	0.09262239504208933
7	0.07047266397822761	0.10212543357345556
8	0.0698257797171898	0.08971895028737192
9	0.068856260718806	0.08959525556778111
10	0.06639711585417954	0.08718805187880971
11	0.06451620612448702	0.0864937827729575
12	0.06393904212692922	0.08712921323180856
13	0.06250488642941822	0.08623539807449988
14	0.05969215175278033	0.09031159861188344
15	0.059854989550358645	0.08542978000675332
16	0.05784734230290985	0.09029548931367717
17	0.056730951114106136	0.0868162489537199
18	0.05408087117301131	0.08343655007197993
19	0.05492635406694855	0.08824550246555911
20	0.05296458030458201	0.0904585966538151
21	0.05107957232588281	0.08480231195111129
22	0.04990102878602391	0.09174773120697419
23	0.04911137756708106	0.08396893604788465
24	0.04725670654425454	0.09128048898318507
25	0.04643657270494397	0.08789759293395517
26	0.04514283334525923	0.09081680665172845
27	0.04396454864393242	0.09378634736854063
28	0.04239133993079039	0.08417922239538649
--------------------------------------------------

--------------------------------------------------
Validation on folder 2:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.43934003557572815	0.20732055463880864
1	0.14492978006670706	0.11348472919853979
2	0.09908755409359599	0.09579471945059749
3	0.08491880384584267	0.09829667406215233
4	0.07953268862997132	0.09184285399720729
5	0.07640421780093366	0.0978643869594583
6	0.07397184497928264	0.09249617448720918
7	0.0729490968150744	0.09376566777439238
8	0.0708777759550226	0.0895963183621753
9	0.06996246228025779	0.10021215928835595
10	0.06847077448032034	0.09257195281661437
11	0.06860911864405793	0.08713761660850274
12	0.06766493524697018	0.08928707455220462
13	0.06656609253621412	0.08593632091047629
14	0.06553794576455206	0.08517449898586708
15	0.06473695454074961	0.09043472354540473
16	0.06376647646131352	0.0845538029540916
17	0.06217115766349729	0.08492572549964834
18	0.061955991921880504	0.10006175397561407
19	0.0603705469858824	0.10737218306569746
20	0.0600052045127533	0.09489020110406685
21	0.05908344189256596	0.09167142904732588
22	0.057292473120389424	0.09964349024208649
23	0.05726124011188246	0.08285732618269492
24	0.056029715743505536	0.08745665589359196
25	0.054729519137207354	0.08230466329428197
26	0.05349624114149221	0.08854058750981242
27	0.05206464002432659	0.08182118725875076
28	0.05124369450190991	0.09025236607511651
29	0.04953198015717599	0.09868221508585057
30	0.04843046588535305	0.0934685607288478
31	0.04723896036502078	0.08493623613092885
32	0.046585586661857935	0.09426272461050243
33	0.0438826937804231	0.0903594983366769
34	0.04315568785767593	0.08982367036313568
35	0.04281318491217256	0.08303881658873749
36	0.04062187270481604	0.07519879840525254
37	0.039225464201839276	0.09078010804541174
38	0.03878564858250667	0.08373365457808166
39	0.035963337137095035	0.08418646423767011
40	0.03558274891428663	0.08059281704115605
41	0.03485747297160896	0.08061700433773815
42	0.033453942846777385	0.08050614937776758
43	0.03241866481606632	0.080602128301753
44	0.031025195072510833	0.07926862289742478
45	0.030399098023910984	0.08699777263140529
46	0.028929896038060305	0.08983872278517335
--------------------------------------------------

--------------------------------------------------
Validation on folder 3:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.45635852162317403	0.20966520836770447
1	0.1502351135388299	0.11262469178292012
2	0.10320028096403562	0.09068350983022727
3	0.0888345361721841	0.08629069336871364
4	0.08360070744677643	0.08227558199883511
5	0.07922319941352587	0.07753571970483744
6	0.07679812655903517	0.07952274325525996
7	0.07596785460060027	0.07730494092571744
8	0.07447517346809643	0.07757096489588695
9	0.07335083407418375	0.0747686741999071
10	0.07078872610715312	0.07653236812618759
11	0.06993084473618737	0.07410865694169899
12	0.0682213799491344	0.07448247785981346
13	0.06830811785947788	0.07092197643017524
14	0.06725321749579712	0.07663343172236775
15	0.06617958533102428	0.07231776981435901
16	0.06496643038590844	0.08047216291652548
17	0.06467633610951394	0.07450577501128502
18	0.06173754204205824	0.07438524409182272
19	0.062110851241319445	0.07271069339796966
20	0.060795463267869124	0.06791622908601856
21	0.059610001347433716	0.07003778638079555
22	0.05880884734604443	0.06780739159406453
23	0.05759617886340092	0.06859369365168438
24	0.05601378370336256	0.06727608084599562
25	0.05451444849312599	0.06921338587917529
26	0.0534358686231602	0.06444935885968442
27	0.052479368202929144	0.0675476256138911
28	0.05016337868431336	0.06513642810256456
29	0.048006140507286	0.07753600184921595
30	0.04773110167883828	0.07020424632355571
31	0.046306336487815	0.06834945824449966
32	0.04487352445933451	0.06661682581395391
33	0.043889280071113486	0.06475813310602911
34	0.042592495161237885	0.06200626682378297
35	0.04137502816718261	0.06430906926160392
36	0.03971815908961761	0.06400075566029043
37	0.03846869215727915	0.06324280342012153
38	0.036882467630055926	0.0675553939343426
39	0.03580253668711256	0.06140186034973389
40	0.035323629893964106	0.06360177538803564
41	0.03447632360504411	0.060499730275870245
42	0.033087754805506665	0.06291157089952396
43	0.032176743385786584	0.0617508545796091
44	0.030383096922434714	0.05981461361928749
45	0.029124009642726018	0.06751444734466612
46	0.02846929033437265	0.06202634303976467
47	0.027284117517929307	0.06568585996213334
48	0.02529030142336451	0.06089552186155422
49	0.02542467036343111	0.059281766781444346
50	0.024280118652025607	0.06635129890823259
51	0.022697394250569422	0.06333652459593601
52	0.02118093325135844	0.06753627220945949
53	0.021888865267228084	0.06246939305547423
54	0.020550846234472345	0.06623141350201434
55	0.019670952888702728	0.06662755078563841
56	0.0185068322873764	0.07127908297942605
57	0.01870138804785557	0.06333124751973718
58	0.01738370118706492	0.06991272405816698
59	0.015917509786821365	0.07254816294548212
--------------------------------------------------

--------------------------------------------------
Validation on folder 4:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.4470741047090459	0.21558436825179464
1	0.1420121941241984	0.12047547133114685
2	0.0986018438633775	0.10007994723486088
3	0.08725781843484005	0.09644931498280619
4	0.0811836007181729	0.08825447203320431
5	0.08000057040948734	0.08447533006269496
6	0.07744755464179494	0.08778030224052191
7	0.07521475679406496	0.0877238337135075
8	0.07412356012283224	0.08274880643853277
9	0.07127003517524104	0.08490580914905381
10	0.07089582651644667	0.08264100353807304
11	0.06889538850670106	0.09170574822027709
12	0.06647151988330428	0.08377881261632539
13	0.06535279266466604	0.08742918334785998
14	0.06548546402306062	0.08010989526177154
15	0.062100737693014545	0.08214807377741802
16	0.0597615296177775	0.08197073246591692
17	0.05829910840438766	0.08328223541298241
18	0.05660668891201788	0.08243663974636849
19	0.05366667910748832	0.08454837863428674
20	0.05234116691208192	0.07917538135609482
21	0.050990433221787354	0.08318545282083123
22	0.048946462389186166	0.08222668779143147
23	0.04649322260989346	0.09009572502871509
24	0.04538238113101955	0.08184559146755435
25	0.04347481994445775	0.08233506963862215
26	0.041917081681367395	0.0837730986438115
27	0.04038646644799509	0.08426675928220291
28	0.038937257451013986	0.08073293348491746
29	0.03615053772656508	0.08389975781618811
30	0.03554242710347905	0.08387657680805345
--------------------------------------------------

--------------------------------------------------
Validation on folder 5:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.45018565078080824	0.23103816677022862
1	0.162386000043771	0.12008529387138508
2	0.10645975829443513	0.09682779781244419
3	0.09130013376215328	0.08852130558755662
4	0.08445675367276866	0.08595968114005195
5	0.08148227488299661	0.08235002360962056
6	0.07837144353019407	0.08289108637306425
7	0.07531093014932139	0.0844098881273358
8	0.07458369069938982	0.08003579920088803
9	0.07248974992180537	0.08376579411051892
10	0.07180195570362342	0.0809910296952283
11	0.0697810203956267	0.07958755189345942
12	0.06841359323260916	0.08133264303207398
13	0.0669655198504471	0.07781917151201655
14	0.0666171576579975	0.07736181854649826
15	0.06575196636284583	0.07337745265828238
16	0.0643570830849107	0.07334962077714778
17	0.06201571578375336	0.07331414986263822
18	0.06160670845506774	0.07067306592508599
19	0.05935838385974356	0.07575393106374476
20	0.05951348169286505	0.07307201056292763
21	0.05742439308829566	0.07092805442158823
22	0.05650684160732285	0.07070360175437397
23	0.05332474540834592	0.0739614522539907
24	0.05298866139681206	0.07786492561576543
25	0.051471807961566876	0.07054820205878329
26	0.049223451505577755	0.07776508365240362
27	0.04779785049576608	0.06789061771498786
28	0.04679221135246668	0.084353843168528
29	0.04696742077065929	0.06809512379544752
30	0.04180996300081018	0.0673178095922426
31	0.040746096121069844	0.06926206041265418
32	0.04143671422298555	0.072754263571567
33	0.04042146274522904	0.06577580405606163
34	0.038002114420938315	0.06974199521872733
35	0.03665045691155691	0.06800813641537119
36	0.035799453192681234	0.08007936536161987
37	0.03409293340272689	0.07123382107251221
38	0.031206353817442016	0.07215633920497365
39	0.03225984703397444	0.07026595251979652
40	0.03062367571154588	0.07526323174712835
41	0.02975163213417317	0.06638404033802174
42	0.02823382450316245	0.06954588477258329
43	0.026862716154094704	0.07175496827397082
--------------------------------------------------


