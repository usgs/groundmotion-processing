
##################################################
Model 3:
	Learning rate:	0.002
	Neuron in HL1:	15
	Neuron in HL2:	20
	Optimizer:	Nadam
	Dropout rate:	0.0
	Activation fx:	sigmoid
	Batch size:	16
	Learning rate decay:	0.0
	L1 regularizer:	0.0
	L2 regularizer:	1e-05
	Patience:	10
--------------------------------------------------
5-fold cross-validation result:	0.08592247849881308
##################################################

--------------------------------------------------
Validation on folder 1:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.5123995890931619	0.30819860412630923
1	0.2160002047617661	0.1599638268005016
2	0.15889327521609373	0.1585526670827422
3	0.14314512134580354	0.1293930930280408
4	0.13272780926636604	0.12007002359212832
5	0.12194614453951062	0.1299605444723437
6	0.1250250767809699	0.11752484232014002
7	0.11721230448253005	0.11478501231257998
8	0.11644582541120428	0.1345550251561542
9	0.11118773082244533	0.10212269925707301
10	0.11103290798179911	0.16488504700016143
11	0.10815352158231346	0.09522462230132417
12	0.10364540958814429	0.13591229103418978
13	0.10285898823194996	0.10055569687121829
14	0.10035172461886974	0.14105808821528457
15	0.09516777649900761	0.09915081009831886
16	0.09675913737195525	0.10224499373675086
17	0.09412937186242089	0.08461496152720133
18	0.09128247326864383	0.09084730671155591
19	0.09322222312351951	0.09222819008539583
20	0.08830507283758127	0.08783772037646105
21	0.09412113005258037	0.09656663197850765
22	0.08979804971306064	0.07851690190389406
23	0.08910174164280595	0.09022894900205523
24	0.08368511113703764	0.08901798575683388
25	0.08166918199093089	0.12987397637131604
26	0.08445386779107644	0.0860145335020714
27	0.08753486251976224	0.07765469000523173
28	0.08397208580635168	0.0804316084831953
29	0.08068736021361583	0.10040002238265304
30	0.08544884943123106	0.07687008788072786
31	0.0835202694950588	0.08539917906962854
32	0.08580239903299207	0.08154547680169344
33	0.0796525336529814	0.07795110795387002
34	0.0783289159103737	0.118404139277287
35	0.07922905160482428	0.08008209865020458
36	0.07431285959377586	0.07470517684597262
37	0.07841764131886485	0.10904603330201881
38	0.08288027124111154	0.08608976090976665
39	0.08060975682876342	0.0695411583495348
40	0.08000443087234282	0.08358312104720363
41	0.08032059259739381	0.10347612536252411
42	0.07596397875704106	0.0759770937567187
43	0.07844497990343495	0.09368292898546125
44	0.07688532366462714	0.11888534286557588
45	0.07740650317140649	0.08677870598296787
46	0.0755373596995642	0.08015209802448056
47	0.07676112006570514	0.07261821651441414
48	0.07328031987837785	0.1070580392816039
49	0.08000780984407083	0.08334195301976315
--------------------------------------------------

--------------------------------------------------
Validation on folder 2:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.5082853217847941	0.2897186237321773
1	0.24782757583992146	0.1997448333552186
2	0.17779354571435427	0.14251216880872217
3	0.15483682286061534	0.12244232309536195
4	0.13645639835927462	0.12102985147019507
5	0.1295411970654657	0.11201550759060282
6	0.12386632430558815	0.10781537537843408
7	0.12659559986059846	0.11929957287412295
8	0.11555483094298882	0.10683770792585023
9	0.11564043368202792	0.10756560979594647
10	0.11186743795347236	0.1046063738809505
11	0.10816945443366947	0.10806163830656401
12	0.10483188841101926	0.14529360868561436
13	0.10613088214110973	0.1068855814530816
14	0.10502649743903415	0.3049161425778564
15	0.1039407123848527	0.1057112509306048
16	0.10209719800265903	0.10206849121711624
17	0.09975660054369524	0.10399159602296185
18	0.09912859458503202	0.11169368830365195
19	0.09995355705330518	0.10799837521684001
20	0.09622535940851439	0.10186551715916312
21	0.09838300980762024	0.1001363251620615
22	0.0928654253281088	0.15971399486904414
23	0.09008758869089374	0.10037478460392482
24	0.09162519725127018	0.09698081860240076
25	0.09156091651687608	0.09579796879224374
26	0.09140511936128193	0.0946082730318459
27	0.0861902541568375	0.09881028462673577
28	0.08900503865964307	0.10156729286405403
29	0.08636326319689355	0.1350026805216158
30	0.08658811378457486	0.09656927751403459
31	0.08504330327041743	0.1036061280315191
32	0.08532378121080812	0.0969654198473608
33	0.08260640167977513	0.09770074895989726
34	0.0856284461454875	0.10716675356240339
35	0.08498751727917941	0.0968286169456764
36	0.08343933216961848	0.09583281463300677
--------------------------------------------------

--------------------------------------------------
Validation on folder 3:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.4849729495854249	0.30952923637319973
1	0.2264523396766368	0.1854792644120042
2	0.16149992200031127	0.16001622939142285
3	0.1430886693445332	0.13376749077512393
4	0.1354343184342544	0.12909951432524325
5	0.1300272883987088	0.11984661692494593
6	0.1252587408435145	0.1226229437040698
7	0.12032547661490624	0.11420192486258879
8	0.11783838291087467	0.10608657374369027
9	0.11239219207699205	0.117913293761356
10	0.11366095507852543	0.12638395590255955
11	0.11274826805563799	0.11654325207182757
12	0.10882147359787246	0.10782000404206543
13	0.10580179885908886	0.10659377110426692
14	0.10242773840462996	0.10176867664834784
15	0.10405017454272104	0.11677903342961615
16	0.10189788901428193	0.11170955087573392
17	0.10008422001783898	0.0925558091265629
18	0.1031625731464934	0.09182598632671528
19	0.09964147907086883	0.10134618248211916
20	0.09888186766742271	0.09287885576486588
21	0.10065391333061505	0.08696637039735142
22	0.0915827747028747	0.10583985437529939
23	0.0988229407270243	0.08434060191588441
24	0.09916281851533548	0.09028040707030154
25	0.09483636873357068	0.08892472220295457
26	0.08951445460015635	0.09155282202590388
27	0.08917567911249222	0.0881174403656406
28	0.08765338197111736	0.10035581564992585
29	0.0904369559203453	0.09106137397463056
30	0.08648255701025559	0.08260046395318385
31	0.09000493306327456	0.08622580446932232
32	0.08722420325482404	0.07880121514160561
33	0.08911336703227327	0.0851925889420899
34	0.08464310489030581	0.08575979171559336
35	0.08233465967077194	0.11771631208362632
36	0.08411186323714838	0.08537529752023863
37	0.08134237706975961	0.0856864334005426
38	0.08578747505754862	0.08124955645089903
39	0.07923355544201666	0.08671742638101045
40	0.0770242194667491	0.0858507564834418
41	0.07954894502505454	0.12037703534968867
42	0.07754269482409182	0.08264684356363333
--------------------------------------------------

--------------------------------------------------
Validation on folder 4:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.5059696054842476	0.33580407865726164
1	0.22865907218780415	0.18107584674645513
2	0.15405989504604198	0.1597233677921939
3	0.13377343506580602	0.14991382790362312
4	0.12219542007433494	0.14590269542251624
5	0.11443619712094288	0.14675444359381268
6	0.11598238593403896	0.1445806498537316
7	0.10690032814784318	0.14278721526591448
8	0.10727492667907892	0.1415973619877422
9	0.10091777727381061	0.1366029720277573
10	0.1029031819702261	0.13409298610785148
11	0.10063063408079856	0.13537274784252157
12	0.10076521279586875	0.13616570995943825
13	0.1014830158073755	0.13289431963306275
14	0.09944460401861313	0.1275983545521315
15	0.0922232771069078	0.13630573128343282
16	0.09365329237151658	0.12461053792550399
17	0.09464527120903694	0.12910040821200305
18	0.09271752747951978	0.13748488469579576
19	0.09386953041973742	0.12502684323525015
20	0.09145141278655136	0.13732170784560432
21	0.08869611827343409	0.11993123143163585
22	0.08922527483622801	0.13773681572604463
23	0.0890170177454812	0.11872733582055917
24	0.09229781848506766	0.13019115180185023
25	0.08459424072600008	0.11990306756941833
26	0.08844274664880818	0.11155935017609575
27	0.0829992414107263	0.13098006557985922
28	0.08126457202213289	0.13336033863389363
29	0.08620754256511311	0.10761002545741244
30	0.08415343359725846	0.12154464611280573
31	0.07855870720425584	0.1133733253307423
32	0.08085319783558573	0.10591177405300041
33	0.07770000837196818	0.10629499645183121
34	0.0789277650175986	0.10658086110178354
35	0.07963353504488421	0.11205586905040554
36	0.07444770623311693	0.10980445673811609
37	0.07660576999720606	0.10552430208024643
38	0.07639690354892947	0.12145718889460512
39	0.07527563616563442	0.10085506602496344
40	0.0763239419164406	0.09909229533080637
41	0.07724022385739257	0.09984240813085632
42	0.07492452698204939	0.13001499610008113
43	0.07354101338598724	0.10991056167119502
44	0.07206000856028899	0.11718240661179498
45	0.07007819487578328	0.12039014905330854
46	0.07108757845334349	0.11229182535061871
47	0.06878053067561925	0.1156521227509871
48	0.07302634682579628	0.09809220694277408
49	0.06807771766548293	0.10100697136218965
50	0.07001756882171725	0.12028577667728992
51	0.06801407877389135	0.09582029919405162
52	0.07014940526596336	0.1192926660961859
53	0.06538762585568193	0.11182638832986572
54	0.06966944567805859	0.11555948838163303
55	0.06905669474704375	0.10514824600400825
56	0.06649673729735112	0.10061691010058144
57	0.06497127111361417	0.10349755519015348
58	0.06523970182175366	0.09631165816299074
59	0.06642832491140353	0.12241179354040184
60	0.06678543726418813	0.10150139187972476
61	0.06405652199267062	0.10848195256978056
--------------------------------------------------

--------------------------------------------------
Validation on folder 5:
--------------------------------------------------
Epoch	Training loss	Validation loss
0	0.5370882681267118	0.322952176538348
1	0.22831899409450493	0.18319727941156333
2	0.16116532001791162	0.17949602537441486
3	0.13856846399877695	0.26178153190722475
4	0.13371194412890144	0.13431842970278332
5	0.12162985688357762	0.12935428992391396
6	0.1182738776663553	0.13019580380965817
7	0.11526730485501169	0.12104923663967111
8	0.11289429837056919	0.39009912721082274
9	0.12162197132905324	0.11011185260351922
10	0.10923640992271591	0.5905177906888844
11	0.139617402475482	0.11912680477280378
12	0.11412658144623691	0.11783492590861218
13	0.1075738656063811	0.1064477749632427
14	0.10698667781861922	0.13795593259912292
15	0.10586161720710266	0.10418293459641538
16	0.10584987209660865	0.1252724358708026
17	0.1014652648081267	0.10153726728767898
18	0.0991946095633785	0.0985544177985906
19	0.0963796565725237	0.10715453617687745
20	0.1013706692226329	0.10011913038882937
21	0.0933062270689355	0.10957424504806036
22	0.09404978231203907	0.11254992175968097
23	0.0965940647928476	0.0921756903083256
24	0.09102390872405158	0.12433866141392072
25	0.09240949866839644	0.12205891389059827
26	0.08751271832317781	0.11418141317546794
27	0.09114188094885228	0.1050751809772458
28	0.08946143652361718	0.17928903285648568
29	0.0875780575744739	0.09376963534670549
30	0.08568164552299105	0.09369762644886712
31	0.08511018375356978	0.09262508571395148
32	0.08203198797362739	0.09084144677702748
33	0.08286698103757072	0.10381686198279307
34	0.08717491298331755	0.10511578309952337
35	0.08295804193989897	0.16493701006652475
36	0.08219875749726249	0.13131289513216973
37	0.08182436722123458	0.17182345994429307
38	0.08080651534520907	0.09851700146043417
39	0.0797809166321316	0.09763383755121846
40	0.08092477086109576	0.0920734893134479
41	0.07587542750845892	0.09735009337313259
42	0.07752790854203048	0.09931678777184129
--------------------------------------------------


